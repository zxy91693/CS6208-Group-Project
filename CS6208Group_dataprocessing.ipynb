{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38e8082b-3448-48ea-841d-507cdd60c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e402c01-fa8e-4c70-a267-93dc1812a718",
   "metadata": {},
   "source": [
    "# Dataset information\n",
    "\n",
    "## Original Dataset\n",
    "https://tianchi.aliyun.com/dataset/dataDetail?dataId=649\n",
    "\n",
    "Random select about 1 million users who have behaviors including click, purchase, adding item to shopping cart and item favoring during November 25 to December 03, 2017. The dataset is organized in a very similar form to MovieLens-20M, i.e. Each line represents a specific user-item interaction, which consists of user ID, item ID, item's category ID, behavior type and timestamp, separated by commas.\n",
    "\n",
    "### Features\n",
    "The detailed descriptions of each field are as follows:\n",
    "\n",
    "|Field     | Explanation |\n",
    "| ----------- | ----------- |\n",
    "|User ID | An integer, the serialized ID that represents a user|\n",
    "|Item ID | An integer, the serialized ID that represents an item|\n",
    "|Category ID | An integer, the serialized ID that represents the category which the corresponding item belongs to|\n",
    "|Behavior type | A string, enum-type from ('pv', 'buy', 'cart', 'fav')|\n",
    "|Timestamp | An integer, the timestamp of the behavior|\n",
    "\n",
    "### Behavior\n",
    "The dataset contains 4 different types of behaviors:\n",
    "\n",
    "|Behavior     | Explanation |\n",
    "| ----------- | ----------- |\n",
    "|pv | Page view of an item's detail page, equivalent to an item click|\n",
    "|buy | Purchase an item|\n",
    "|cart | Add an item to shopping cart|\n",
    "|fav | Favor an item|\n",
    "\n",
    "### Size\n",
    "Dimensions of the dataset are:\n",
    "\n",
    "|Dimension    |  Number  |\n",
    "| ----------- | ----------- |\n",
    "|# of users | 987,994|\n",
    "|# of items | 4,162,024|\n",
    "|# of categories | 9,439|\n",
    "|# of interactions | 100,150,807|\n",
    "\n",
    "## Processed Dataset\n",
    "1. Extract November 25, 2017 interactions information from original dataset.\n",
    "2. Treat all interactions as positive interactions\n",
    "3. Reserve the earlier interaction record of each user-item pair.\n",
    "4. Drop top 0.5% items \n",
    "5. Iteratively drop sparse users and items until all users and items have 10 more interactions\n",
    "6. Valid and Test dataset only contains warm users and warm items of Train dataset\n",
    "\n",
    "### Size\n",
    "Dimensions of the dataset are:\n",
    "\n",
    "|Type|# of users|# of items|# of interactions|\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "|Train|51635|44238|687600|\n",
    "|Valid|22792|39958|131125|\n",
    "|Test|19529|38382|111701|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52f12a1f-4953-44ff-9ffa-0e98900bb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_uipair_dict(df):\n",
    "    df_temp = df.copy()[['user_id','item_id']]\n",
    "    df_temp = df_temp.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "    return df_temp\n",
    "\n",
    "\n",
    "def count_data_info(df):\n",
    "    statistic_dict = {}\n",
    "    for i in ['user_id', 'item_id']:\n",
    "        data_feature_num = df[i].value_counts(normalize = False, dropna = True).to_dict()        \n",
    "        statistic_dict[i] = data_feature_num\n",
    "    statistic_dict['interactions'] = np.sum(np.asarray(np.array(list(statistic_dict['user_id'].items()))[:,1], dtype = int))\n",
    "    return statistic_dict\n",
    "\n",
    "\n",
    "def warm_user_warm_item(df,train_statistic_dict):\n",
    "    df_out = df[(df['user_id'].isin(train_statistic_dict['user_id'])) & (df['item_id'].isin(train_statistic_dict['item_id']))].copy()\n",
    "    df_out.reset_index(drop = True,inplace = True)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def df_to_uipair_txt(df,ui_to_id_dict, traget_path):\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    df_temp['user_id'] = df_temp['user_id'].apply(lambda x:ui_to_id_dict['user_id'][x])\n",
    "    df_temp['item_id'] = df_temp['item_id'].apply(lambda x:ui_to_id_dict['item_id'][x])\n",
    "\n",
    "    df_temp = df_temp.astype(str)[['user_id','item_id']]\n",
    "    df_temp = df_temp.groupby(['user_id'], as_index=False).apply(lambda x: '\\t'.join(x['item_id'])).reset_index(drop=True)\n",
    "    df_temp.to_csv(traget_path, header=None, index=False, sep='\\t', quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "\n",
    "\n",
    "def remove_top_value(df_uni,top):\n",
    "    topK = top\n",
    "    item_num = df_uni.groupby('item_id',as_index=False).count().shape[0]\n",
    "    pop_item = df_uni.groupby('item_id',as_index=False).count().sort_values('user_id',ascending=False).head(int(item_num*topK/100))['item_id'].tolist()\n",
    "    print('popular item num:',len(pop_item))\n",
    "\n",
    "    df_rmpop = df_uni[~df_uni['item_id'].isin(pop_item)]\n",
    "    return df_rmpop\n",
    "\n",
    "\n",
    "def select_kcore(interaction_dict,K=3):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        item_cnt_dict = {}\n",
    "        item_drop_dict = {}\n",
    "        # create item_drop_dict, item_cnt_dict\n",
    "        for user_id in interaction_dict:\n",
    "            for item_id in interaction_dict[user_id]:\n",
    "                if item_id not in item_cnt_dict:\n",
    "                    item_cnt_dict[item_id] = 0\n",
    "                    item_drop_dict[item_id] = 0\n",
    "                item_cnt_dict[item_id] += 1\n",
    "\n",
    "        #print('user num:',len(interaction_dict))\n",
    "        assert len(item_drop_dict)==len(item_cnt_dict)\n",
    "\n",
    "        # delete items < K\n",
    "        del_iid_list = []\n",
    "        for item_id in item_cnt_dict:\n",
    "            if item_cnt_dict[item_id] < K:\n",
    "                del_iid_list.append(item_id)\n",
    "\n",
    "        for item_id in del_iid_list:\n",
    "            item_drop_dict[item_id] = 1\n",
    "        for user_id in interaction_dict:\n",
    "            del_id_list = []\n",
    "            for item_id in interaction_dict[user_id]:\n",
    "                if item_drop_dict[item_id]:\n",
    "                    del_id_list.append(item_id)\n",
    "            for del_id in del_id_list:\n",
    "                if del_id in interaction_dict[user_id]:\n",
    "                    interaction_dict[user_id].remove(del_id)\n",
    "\n",
    "        item_drop_num = 0\n",
    "        for item_id in item_drop_dict:\n",
    "            item_drop_num += item_drop_dict[item_id]\n",
    "        item_num = len(item_drop_dict) - item_drop_num\n",
    "        #print(f'item num after item-{K}core:',item_num)\n",
    "\n",
    "        new_item_cnt = {}\n",
    "        min_cnt=999\n",
    "        for user_id in interaction_dict:\n",
    "            min_cnt = min(min_cnt, len(interaction_dict[user_id]))\n",
    "            for item_id in interaction_dict[user_id]:\n",
    "                if item_id not in new_item_cnt:\n",
    "                    new_item_cnt[item_id] = 0\n",
    "                new_item_cnt[item_id] += 1\n",
    "        #print('min user interaction:',min_cnt)\n",
    "        min_cnt_item = 999\n",
    "        for item_id in new_item_cnt:\n",
    "            min_cnt_item = min(min_cnt_item, new_item_cnt[item_id])\n",
    "        #print('min item num:',min_cnt_item)\n",
    "        if min_cnt>=K and min_cnt_item>=K:\n",
    "            return interaction_dict, len(interaction_dict), item_num\n",
    "        \n",
    "        # delete users interactions<K\n",
    "        del_uid_list = []\n",
    "        for user_id in interaction_dict:\n",
    "            if len(interaction_dict[user_id])<K:\n",
    "                del_uid_list.append(user_id)\n",
    "        for user_id in del_uid_list:\n",
    "            del interaction_dict[user_id]\n",
    "        \n",
    "        # count min user-interaction and item appearance\n",
    "        new_item_cnt = {}\n",
    "        min_cnt=999\n",
    "        for user_id in interaction_dict:\n",
    "            min_cnt = min(min_cnt, len(interaction_dict[user_id]))\n",
    "            for item_id in interaction_dict[user_id]:\n",
    "                if item_id not in new_item_cnt:\n",
    "                    new_item_cnt[item_id] = 0\n",
    "                new_item_cnt[item_id] += 1\n",
    "        min_cnt_item = 999\n",
    "        for item_id in new_item_cnt:\n",
    "            min_cnt_item = min(min_cnt_item, new_item_cnt[item_id])\n",
    "            \n",
    "        if min_cnt>=K and min_cnt_item>=K:\n",
    "            return interaction_dict, len(interaction_dict), item_num\n",
    "\n",
    "\n",
    "def remove_sparse_interaction(df,uipair_dict,K=3):\n",
    "    interaction_dict, not_sparse_user, not_sparse_item = select_kcore(uipair_dict,K)\n",
    "    \n",
    "    not_sparse_dict = {'user_id':set([]),'item_id':set([])}\n",
    "    \n",
    "    for k,v in interaction_dict.items():\n",
    "        not_sparse_dict['user_id'].add(k)\n",
    "        for i in v:\n",
    "            not_sparse_dict['item_id'].add(i)\n",
    "\n",
    "    df_remove_sparse = df[(df['user_id'].isin(not_sparse_dict['user_id'])) & (df['item_id'].isin(not_sparse_dict['item_id']))].copy()\n",
    "    return df_remove_sparse, not_sparse_dict\n",
    "\n",
    "\n",
    "def save_ui_dict(not_sparse_dict,target_path):\n",
    "    ui_to_id_dict = {'user_id':{},'item_id':{}}\n",
    "\n",
    "    for user in list(not_sparse_dict['user_id']):\n",
    "        if user not in ui_to_id_dict['user_id'].keys():\n",
    "            ui_to_id_dict['user_id'][user] = len(ui_to_id_dict['user_id'])\n",
    "\n",
    "    for item in list(not_sparse_dict['item_id']):\n",
    "        if item not in ui_to_id_dict['item_id'].keys():\n",
    "            ui_to_id_dict['item_id'][item] = len(ui_to_id_dict['item_id'])\n",
    "            \n",
    "    dict_file = open(target_path + 'ui_to_id.dict', 'wb') \n",
    "    pickle.dump(ui_to_id_dict,dict_file)\n",
    "    dict_file.close()\n",
    "\n",
    "    id_to_ui_dict = {'user_id':{v:k for k,v in ui_to_id_dict['user_id'].items()},'item_id':{v:k for k,v in ui_to_id_dict['item_id'].items()}}\n",
    "    \n",
    "    dict_file = open(target_path + 'id_to_ui.dict', 'wb') \n",
    "    pickle.dump(id_to_ui_dict,dict_file)\n",
    "    dict_file.close()\n",
    "    \n",
    "    dict_file = open(target_path + 'dataset_size.txt', 'w') \n",
    "    dict_file.write(\"{} {}\".format(str(len(ui_to_id_dict['user_id'])),str(len(ui_to_id_dict['item_id']))))\n",
    "    dict_file.close()\n",
    "    \n",
    "    return ui_to_id_dict,id_to_ui_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505bcc05-363a-45f3-bb5d-1567df0fc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bc6891-d0c5-407f-bf6f-6d87fc968f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(base_path + 'data/UserBehavior.csv',encoding='utf')\n",
    "data.columns=['user_id','item_id','category_id','behavior','time']\n",
    "\n",
    "def num_to_time(x):\n",
    "    return datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data['time'] = data['time'].apply(num_to_time)\n",
    "data['time'] = pd.to_datetime(data['time'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdf8da2-0d5e-4f4f-ae3e-9b307216f5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>behavior</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>2017-11-25 06:15:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>2017-11-25 09:21:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>2017-11-25 15:04:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>2017-11-25 15:49:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4606018</td>\n",
       "      <td>2735466</td>\n",
       "      <td>pv</td>\n",
       "      <td>2017-11-25 21:28:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  category_id behavior                time\n",
       "0        1  2333346      2520771       pv 2017-11-25 06:15:33\n",
       "1        1  2576651       149192       pv 2017-11-25 09:21:25\n",
       "2        1  3830808      4181361       pv 2017-11-25 15:04:53\n",
       "3        1  4365585      2520377       pv 2017-11-25 15:49:06\n",
       "4        1  4606018      2735466       pv 2017-11-25 21:28:01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840ba047-3d19-4bb3-bb23-3c3bfd82f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp('2017-11-25 00:00:00')\n",
    "end_time = pd.Timestamp('2017-11-26 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f49d094-3512-4d15-b1d9-010025800d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledata = data[(data['time']>=start_time) & (data['time']<end_time)][['user_id','item_id','time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed09da28-8515-4266-9816-48337755b6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10420014, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d69e3ff-fc9f-479a-8317-23163e9c6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_id_value_counts = sampledata['user_id'].value_counts()\n",
    "sample_item_id_value_counts = sampledata['item_id'].value_counts()\n",
    "print(sample_user_id_value_counts,sample_item_id_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ec2a1d5-7846-4829-b888-a23d12f7e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = sampledata.groupby(['user_id', 'item_id'], as_index=False)['time'].min()\n",
    "unique_data = unique_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47349828-d23d-4d43-a6ae-edfd5477e24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8786839, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf415f-cb0b-40ca-ad3a-0cfeb470a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_data.to_csv(base_path + 'data/1days_unique_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ca966f7a-48c1-45fe-af41-cdfc2d54fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data=pd.read_csv(base_path + 'data/1days_unique_data.csv',encoding='utf',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92ea8f2c-b434-4d2f-a5ab-f719b0367095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8786839, 3)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc569d-28ef-479b-8746-c5a3510eed37",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_user_id_value_counts = unique_data['user_id'].value_counts()\n",
    "sample_item_id_value_counts = unique_data['item_id'].value_counts()\n",
    "print(sample_user_id_value_counts,sample_item_id_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "38ab1093-ced2-43ec-98e2-740c26e7aa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8786839, 3)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "44820382-6254-4828-ab06-7f9ae8b2069b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top = 0.3\n",
    "sparse = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "80edac32-65f4-443e-9587-93d5f0e96554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular item num: 4834\n"
     ]
    }
   ],
   "source": [
    "remove_top_data = remove_top_value(unique_data,top)\n",
    "remove_top_uipair_dict = df_to_uipair_dict(remove_top_data)\n",
    "df_remove_top_sparse, not_sparse_dict = remove_sparse_interaction(remove_top_data,remove_top_uipair_dict,sparse)\n",
    "ui_to_id_dict_sparse,id_to_ui_dict_sparse = save_ui_dict(not_sparse_dict,base_path+'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4408bb96-9dda-45a4-b20f-44b8cb109b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989989, 3)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_top_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5cb36bed-729a-48fe-92e2-2f1c8e0f780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_remove_top_sparse.to_csv(base_path + 'data/1day_unique_Remove_top{}_sparse{}.csv'.format(str(top),str(sparse))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71737301-5c96-4e83-9a87-4fb07d58b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_top_sparse=pd.read_csv(base_path + 'data/1day_unique_Remove_top{}_sparse{}.csv'.format(str(top),str(sparse)),encoding='utf',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a8b74-8a29-4803-b7ac-198697f7e4cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_user_id_value_counts = df_remove_top_sparse['user_id'].value_counts()\n",
    "sample_item_id_value_counts = df_remove_top_sparse['item_id'].value_counts()\n",
    "print(sample_user_id_value_counts,sample_item_id_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "852aa810-01a1-4765-9bd9-cdb7b09fdb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_remove_top_sparse['time'] = pd.to_datetime(df_remove_top_sparse['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6ebacae8-dcd0-4068-97e9-ca4c8dbb3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_info_dict = {}\n",
    "train_stage = ['stage_'+str(i) for i in range(10)]\n",
    "valid_stage = ['stage_'+str(i) for i in range(8,10)]\n",
    "test_stage = ['stage_'+str(i) for i in range(9,11)]\n",
    "\n",
    "stage_info_dict['train'] = train_stage\n",
    "stage_info_dict['valid'] = valid_stage\n",
    "stage_info_dict['test'] = test_stage\n",
    "\n",
    "statistic_dict_file = open(base_path + 'data/stage_info_dict.dict', 'wb') \n",
    "pickle.dump(stage_info_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7c57a-3904-438c-ad4f-e12e53f722ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "184e843d-1297-4a00-a98b-4a0645256ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_to_stage_map_dict = {}\n",
    "stage_cnt = 0\n",
    "for hour in range(22):\n",
    "    hour_to_stage_map_dict[hour] = 'stage_' + str(int(stage_cnt/2))\n",
    "    stage_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5381b7-b09a-465e-8e13-2f34c17f5754",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hour_to_stage_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b2aa94dc-9b23-43a3-93e3-15bb8c91f0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating stage_item_popularity_dict\n"
     ]
    }
   ],
   "source": [
    "stage_time = [(pd.Timestamp('2017-11-25 00:00:00')+pd.Timedelta(minutes=120*i),pd.Timestamp('2017-11-25 1:59:59')+pd.Timedelta(minutes=120*i)) for i in range(11)]\n",
    "\n",
    "if os.path.exists(base_path + 'data/stage_item_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(base_path + 'data/stage_item_popularity_dict.dict','rb') as f:\n",
    "        stage_item_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating stage_item_popularity_dict')\n",
    "    stage_item_popularity_dict = {}\n",
    "    for i in range(len(stage_time)):\n",
    "        start_time = stage_time[i][0]\n",
    "        end_time = stage_time[i][1]\n",
    "        df_onestage = df_remove_top_sparse[['user_id','item_id']][(df_remove_top_sparse['time']>=start_time) & (df_remove_top_sparse['time'] <= end_time)].copy()\n",
    "        iid_inter_count_onestage = df_onestage.groupby('item_id',as_index=False).count()\n",
    "        iid_inter_count_onestage = iid_inter_count_onestage[['user_id','item_id']]\n",
    "        iid_inter_count_onestage.rename(columns={'item_id': 'item_id', 'user_id': 'count_'+ str(i)}, inplace=True)\n",
    "        interactions_onestage = iid_inter_count_onestage['count_'+ str(i)].sum()\n",
    "        iid_inter_count_onestage['count_'+ str(i)] = iid_inter_count_onestage['count_'+ str(i)]/interactions_onestage\n",
    "        iid_inter_count_onestage.set_index(['item_id'], inplace = True)\n",
    "        stage_item_popularity_dict['stage_'+str(i)] = iid_inter_count_onestage.to_dict('dict')['count_'+ str(i)]\n",
    "    \n",
    "    for k,v in stage_item_popularity_dict.items():\n",
    "        deno_normal = max(list(v.values())) - min(list(v.values()))\n",
    "        for k1,v1 in v.items():\n",
    "            stage_item_popularity_dict[k][k1] = v1/deno_normal\n",
    "    \n",
    "    statistic_dict_file = open(base_path + 'data/stage_item_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(stage_item_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caaaca-349d-414c-97c9-f4f45781c9f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage_item_popularity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bb570491-c012-4ccf-add2-d4c8c1348396",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2017-11-25 00:00:00'), Timestamp('2017-11-25 01:59:59')),\n",
       " (Timestamp('2017-11-25 02:00:00'), Timestamp('2017-11-25 03:59:59')),\n",
       " (Timestamp('2017-11-25 04:00:00'), Timestamp('2017-11-25 05:59:59')),\n",
       " (Timestamp('2017-11-25 06:00:00'), Timestamp('2017-11-25 07:59:59')),\n",
       " (Timestamp('2017-11-25 08:00:00'), Timestamp('2017-11-25 09:59:59')),\n",
       " (Timestamp('2017-11-25 10:00:00'), Timestamp('2017-11-25 11:59:59')),\n",
       " (Timestamp('2017-11-25 12:00:00'), Timestamp('2017-11-25 13:59:59')),\n",
       " (Timestamp('2017-11-25 14:00:00'), Timestamp('2017-11-25 15:59:59')),\n",
       " (Timestamp('2017-11-25 16:00:00'), Timestamp('2017-11-25 17:59:59')),\n",
       " (Timestamp('2017-11-25 18:00:00'), Timestamp('2017-11-25 19:59:59')),\n",
       " (Timestamp('2017-11-25 20:00:00'), Timestamp('2017-11-25 21:59:59'))]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a413ba0c-343a-42f2-aa99-eb67065ace99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating stage_iid_popularity_dict\n"
     ]
    }
   ],
   "source": [
    "print('generating stage_iid_popularity_dict')\n",
    "stage_iid_popularity_dict = {}\n",
    "for k,v in stage_item_popularity_dict.items():\n",
    "    stage_iid_popularity_dict[k] = {}\n",
    "    for k1,v1 in v.items():\n",
    "        stage_iid_popularity_dict[k][ui_to_id_dict_sparse['item_id'][k1]] = v1\n",
    "\n",
    "statistic_dict_file = open(base_path + 'data/stage_iid_popularity_dict.dict', 'wb') \n",
    "pickle.dump(stage_iid_popularity_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaefa4f-dce2-4f36-be41-8268028e42f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage_iid_popularity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2e3ecb2a-d2d2-4843-8979-e59c0c9e2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989989, 3)\n",
      "generating stage_item_popularity_dict\n"
     ]
    }
   ],
   "source": [
    "temp_df_uidate_popular = df_remove_top_sparse[['user_id','item_id','time']]\n",
    "print(temp_df_uidate_popular.shape)\n",
    "\n",
    "if os.path.exists(base_path + 'data/mappedui_to_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(base_path + 'data/mappedui_to_popularity_dict.dict','rb') as f:\n",
    "        mappedui_to_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating mappedui_to_popularity_dict')\n",
    "    mappedui_to_popularity_dict = {}\n",
    "    for idx, row in temp_df_uidate_popular.iterrows():\n",
    "        if row['time'].hour in hour_to_stage_map_dict:\n",
    "            ui_index = (ui_to_id_dict_sparse['user_id'][row['user_id']],ui_to_id_dict_sparse['item_id'][row['item_id']])\n",
    "            mappedui_to_popularity_dict[ui_index] = stage_item_popularity_dict[hour_to_stage_map_dict[row['time'].hour]][row['item_id']]\n",
    "\n",
    "    statistic_dict_file = open(base_path + 'data/mappedui_to_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(mappedui_to_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166d5ec-b919-42e5-a2cb-caf678d8d731",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mappedui_to_popularity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f364d584-0e05-401f-9346-c478d0f058ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989989, 3)\n",
      "generating mappedui_stage_popularity_dict\n"
     ]
    }
   ],
   "source": [
    "temp_df_uidate_popular = df_remove_top_sparse[['user_id','item_id','time']]\n",
    "print(temp_df_uidate_popular.shape)\n",
    "\n",
    "if os.path.exists(base_path + 'data/mappedui_stage_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(base_path + 'data/mappedui_stage_popularity_dict.dict','rb') as f:\n",
    "        mappedui_stage_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating mappedui_stage_popularity_dict')\n",
    "    mappedui_stage_popularity_dict = {}\n",
    "    for idx, row in temp_df_uidate_popular.iterrows():\n",
    "        if row['time'].hour in hour_to_stage_map_dict:\n",
    "            ui_index = (ui_to_id_dict_sparse['user_id'][row['user_id']],ui_to_id_dict_sparse['item_id'][row['item_id']])\n",
    "            mappedui_stage_popularity_dict[ui_index] = hour_to_stage_map_dict[row['time'].hour]\n",
    "\n",
    "    statistic_dict_file = open(base_path + 'data/mappedui_stage_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(mappedui_stage_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69336a3-498f-4a9d-9041-267cc4abd744",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mappedui_stage_popularity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "419ed2c5-a143-40a1-815c-610d974d420a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_start_time =  pd.Timestamp('2017-11-25 00:00:00')\n",
    "train_end_time = pd.Timestamp('2017-11-25 19:59:59')\n",
    "val_start_time = pd.Timestamp(\"2017-11-25 20:00:00\")\n",
    "val_end_time = pd.Timestamp(\"2017-11-25 21:59:59\")\n",
    "test_start_time = pd.Timestamp(\"2017-11-25 22:00:00\")\n",
    "test_end_time = pd.Timestamp(\"2017-11-25 23:59:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "573c8010-38d9-4c84-8331-75de8c56b638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train:\n",
      "interactions: 687600 \n",
      "users: 51635 \n",
      "items: 44238\n",
      "\n",
      "val_wuwi:\n",
      "interactions: 131125 \n",
      "users: 22792 \n",
      "items: 39958\n",
      "\n",
      "test_wuwi:\n",
      "interactions: 111701 \n",
      "users: 19529 \n",
      "items: 38382\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "df_train = df_remove_top_sparse[(df_remove_top_sparse['time']>=train_start_time) & (df_remove_top_sparse['time']<=train_end_time)].copy()\n",
    "\n",
    "df_train_statistic_dict = count_data_info(df_train)\n",
    "print()\n",
    "print(\"train:\")\n",
    "print(\"interactions:\",df_train_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_train_statistic_dict['user_id']),\"\\nitems:\",len(df_train_statistic_dict['item_id']))\n",
    "\n",
    "df_to_uipair_txt(df_train, ui_to_id_dict_sparse, base_path + 'data/train.txt')\n",
    "\n",
    "\n",
    "# valid\n",
    "# valid warm user warm item\n",
    "df_val = df_remove_top_sparse[(df_remove_top_sparse['time']>=val_start_time) & (df_remove_top_sparse['time']<=val_end_time)].copy()\n",
    "\n",
    "df_wuwi_val = warm_user_warm_item(df_val, df_train_statistic_dict)\n",
    "\n",
    "df_val_statistic_dict = count_data_info(df_wuwi_val)\n",
    "print()\n",
    "print(\"val_wuwi:\")\n",
    "print(\"interactions:\",df_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_val_statistic_dict['user_id']),\"\\nitems:\",len(df_val_statistic_dict['item_id']))\n",
    "\n",
    "df_to_uipair_txt(df_wuwi_val, ui_to_id_dict_sparse, base_path + 'data/valid.txt')\n",
    "\n",
    "\n",
    "# test\n",
    "df_test = df_remove_top_sparse[(df_remove_top_sparse['time']>=test_start_time) & (df_remove_top_sparse['time']<=test_end_time)].copy()\n",
    "\n",
    "# valid warm user warm item\n",
    "df_wuwi_test = warm_user_warm_item(df_test, df_train_statistic_dict)\n",
    "\n",
    "df_test_statistic_dict = count_data_info(df_wuwi_test)\n",
    "print()\n",
    "print(\"test_wuwi:\")\n",
    "print(\"interactions:\",df_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_test_statistic_dict['user_id']),\"\\nitems:\",len(df_test_statistic_dict['item_id']))\n",
    "\n",
    "df_to_uipair_txt(df_wuwi_test, ui_to_id_dict_sparse, base_path + 'data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7a106003-9b03-4f9f-9027-bab9f5dc387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989989, 3)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_top_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ab7ca9ed-4abc-40a6-bf83-65bf9ad07739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930426"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "687600+131125+111701"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
